I"œ <p>While I agree fully with many of the conclusions Lukas Mathis draws in an <a href="http://ignorethecode.net/blog/2009/05/10/measuring-the-user-interface/" title="Lukas Mathis: 'Measuring the User Interface'">excellent essay on the recent Google/Douglas Bowman split</a>, a few bits deserve further study.  Lukas hits the central problem squarely on the head in his <a href="http://ignorethecode.net/blog/2009/05/10/measuring-the-user-interface/#fn:google">footnoted claim</a> that measurement isnâ€™t Googleâ€™s problem, rather the lack of a design team capable of <em>balancing</em> those measurements against other concerns.  Iâ€™d elaborate on this claim, and argue that the <em>kind</em> of testing being discussed matters.  In general, engineers understand and can relate well to automated A/B testing, and designers understand and can relate well to more personal usability testing.  The two are, however, not the same, donâ€™t provide the same data, and ought not be conflated.</p>

<h2 id="what-do-ab-tests-test">What do A/B tests test?</h2>

<p>â€œ41 shades of blueâ€ has become a catchphrase for those who see Googleâ€™s approach as flawed; this seemingly mindless insistence on A/B testing as the sole arbiter of a designâ€™s â€œvalueâ€ seems to me to be the point Bowman most objected to.  I have a good amount of sympathy for that viewpoint.</p>

<p>A/B testing is, as Lukas rightly points out, hugely valuable to any team focused on improving an established system through a series of small, iterative changes.  Serving a green button in place of a red button to 1% of your users, and discovering a statistically meaningful shift in usage patterns distinctly tells you something about your audience that no amount of surveying could.  Ignoring these userâ€™s <em>claims</em> while carefully observing their <em>behavior</em> can reveal interesting and useful bits of data about the way they interact with the interfaces youâ€™ve built.</p>

<p>Especially with an audience the size of Googleâ€™s, youâ€™d have to be crazy to write off the value of building small changes incrementally, and testing those piecemeal improvements against a subset of your audience.  Youâ€™ll discover that one small tweak or another has an impact, and feed that back into your design process.  Continually testing each iteration of tweaks against your audience will continually improve the quality of your pages and the overall user experience:  Itâ€™s a virtuous circle of exactly the kind Iâ€™d like to encourage.</p>

<p>I absolutely stand behind the core of Lukasâ€™ essay: â€œDesign is not held back by data, design is refined and perfected by data.â€</p>

<h2 id="the-downside">The Downside</h2>

<p>A/B testing is appealing because itâ€™s obvious.  Itâ€™s absolutely clear what needs to be done, the results are generally clear in the direction they lead (itâ€™s either â€œAâ€ or â€œBâ€), and if you have the tooling in place to make it easy (Google does), itâ€™s easy to see how it can become a central part of the way you work.</p>

<p>That said, youâ€™d be just as crazy to make <em>every</em> decision dependent on this sort of testing.  In order to isolate the effect of a change, as a true engineer-statistician ought demand, broad and sweeping suggestions should be broken down into testable components, and each served across a just-wide-enough swath of audience to return meaningful data while minimizing risk.  An insistance on A/B measurement of each change, no matter how small, means that <em>everything</em> you measure will be â€œtrivialâ€ in the way that the most glaring examples of â€œGooglismâ€ have been: shades of colour, or widths of border.  Only <em>trivial</em> changes are <em>testable</em> changes, everything else is fraught with layers of uncertainty: â€œWas it the new 3px border, or the new position of the button, or the completely new logo that drove traffic?â€ â€œWho knows, letâ€™s do more tests!â€</p>

<p>Moreover, itâ€™s easy to see how a reliance on A/B testing might lead to a culture of extreme risk-aversion.  When you have a ton of data at your fingertips that help guide your decisions, your worst enemy is the black hole that a new untested/untestable/unmeasured change represents.  If a change isnâ€™t testable/tested, itâ€™s unknown.  Unknown changes might be catastrophic.  Catastrophic is bad.  Therefore untested/untestable/unmeasured changes are, in the absence of <em>evidence</em>, bad.</p>

<p>I suspect that Bowman isnâ€™t at all exhorting his fellow designers to abstain completely from testing; Iâ€™d interpret his remarks as cutting against an <em>over-reliance</em> on A/B testing.  Within a primarily A/B framework, Iâ€™d consider it impossible to do <em>real</em> design work that reevaluates a productâ€™s decisions from first principles.  Where can you begin?  What interesting set of changes can you propose when each has to be buildable in a way that allows testing against the current status quo of the product as a baseline?</p>

<p>I believe that Bowman is right to worry about missing â€œtruly greatâ€ designs by getting bogged down in continual, iterative improvement of a mediocre design.  Getting wrapped up in asymptoticly approaching the <em>best</em> shade of the <em>best</em> button you can possibly create, and backing it all up with testing, might blind you to the fact that the action might really be better represented as a slider.  Or a dropdown box.  Or that itâ€™s completely wrong and would be better reworked entirely.</p>

<p>Lukas quickly notes one answer in passing.  I think it deserves more attention.</p>

<h2 id="usability-testing-and-prototypes">Usability Testing and Prototypes</h2>

<p>In an <a href="http://machinist.salon.com/blog/2008/06/09/leander_kahney/index.html">interview with Salon in 2008</a>, Leander Kahney said:</p>

<blockquote>Steve Jobs doesn't wake up one morning and there's a vision of an iPhone floating in front of his face. He and his team discovered it through this exhaustive process of building prototype after prototype.</blockquote>

<p>Each of these prototypes functioned in one way or another, and each was put into someoneâ€™s hands to play with.  This methodology of testing (especially for <em>new</em> designs) is simply not measurable in the same way as determining whether a green link on a pre-existing page is clicked on more or fewer times than the same link in red.  A/B testing <em>is not</em> usability testing sweeping new prototypes; it has a different purpose, and gives different data points.</p>

<p>Prototyping new solutions to problems, and putting them in front of real people as quickly as possible, is probably the best way to make sweeping changes while maintaining some semblance of confidence that youâ€™re moving in the right direction with regard to the audience youâ€™re aiming for.</p>

<p>This sort of guerilla testing designs on friends and acquaintances with something like <a href="http://ignorethecode.net/blog/2008/08/04/silverback/">Silverback</a> absolutely ought to be part of the design process, early and often.  Getting real feedback from people who <em>use</em> the system youâ€™re trying to improve is critical to designing and implementing an interface that makes the right tradeoffs in the right places.</p>

<p>In short, Iâ€™m highly sympathetic to the <a href="https://twitter.com/iA/statuses/1752478005">Nielsenian notion</a> that users <em>canâ€™t</em> and <em>wonâ€™t</em> tell us what they want, that we need to examine their clicks in order to measure the impact of the changes we make.  However, I think that notion needs to be tempered insofar as <em>never</em> asking a user what she thinks is incredibly dangerous because it never allows you to break out of a certain comfort zone surrounding the product as it stands.</p>

<p>So.  Test.  Early, often, and persistently.  Testing gives you immense amounts of data, and data is invaluable to your designs and implementations.  But donâ€™t limit your tests to those best expressed in a spreadsheet.  Prototyping, and asking actual users actual questions, should be a part of your creative process.</p>

<p>Finally, to reiterate Lukasâ€™ point: you arenâ€™t Google.  Even if theyâ€™re dangerously-reliant on A/B testing, you almost certainly arenâ€™t A/B testing <em>enough</em>.  I know Iâ€™m notâ€¦</p>

:ET